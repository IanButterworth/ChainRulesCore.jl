<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Introduction · ChainRules</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/><link href="assets/chainrules.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href="index.html"><img class="logo" src="assets/logo.svg" alt="ChainRules logo"/></a><h1>ChainRules</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search.html"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href="index.html">Introduction</a><ul class="internal"><li><a class="toctext" href="#Introduction-1">Introduction</a></li><li><a class="toctext" href="#frule-and-rrule-1"><code>frule</code> and <code>rrule</code></a></li><li><a class="toctext" href="#The-propagators:-pushforward-and-pullback-1">The propagators: pushforward and pullback</a></li><li><a class="toctext" href="#Differentials-1">Differentials</a></li><li><a class="toctext" href="#Example-of-using-ChainRules-directly.-1">Example of using ChainRules directly.</a></li></ul></li><li><a class="toctext" href="FAQ.html">FAQ</a></li><li><a class="toctext" href="writing_good_rules.html">Writing Good Rules</a></li><li><a class="toctext" href="debug_mode.html">Debug Mode</a></li><li><span class="toctext">Design</span><ul><li><a class="toctext" href="design/many_differentials.html">Many Differential Types</a></li></ul></li><li><a class="toctext" href="api.html">API</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href="index.html">Introduction</a></li></ul><a class="edit-page" href="https://github.com/JuliaDiff/ChainRulesCore.jl/blob/master/docs/src/index.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Introduction</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="ChainRules-1" href="#ChainRules-1">ChainRules</a></h1><p><a href="https://github.com/JuliaDiff/ChainRules.jl">ChainRules</a> provides a variety of common utilities that can be used by downstream <a href="https://en.wikipedia.org/wiki/Automatic_differentiation">automatic differentiation (AD)</a> tools to define and execute forward-, reverse-, and mixed-mode primitives.</p><h2><a class="nav-anchor" id="Introduction-1" href="#Introduction-1">Introduction</a></h2><p>ChainRules is all about providing a rich set of rules for differentiation. When a person learns introductory calculus, they learn that the derivative (with respect to <code>x</code>) of <code>a*x</code> is <code>a</code>, and the derivative of <code>sin(x)</code> is <code>cos(x)</code>, etc. And they learn how to combine simple rules, via <a href="https://en.wikipedia.org/wiki/Chain_rule">the chain rule</a>, to differentiate complicated functions. ChainRules is a programmatic repository of that knowledge, with the generalizations to higher dimensions.</p><p><a href="https://en.wikipedia.org/wiki/Automatic_differentiation">Autodiff (AD)</a> tools roughly work by reducing a problem down to simple parts that they know the rules for, and then combining those rules. Knowing rules for more complicated functions speeds up the autodiff process as it doesn&#39;t have to break things down as much.</p><p><strong>ChainRules is an AD-independent collection of rules to use in a differentiation system.</strong></p><div class="admonition note"><div class="admonition-title">The whole field is a mess for terminology</div><div class="admonition-text"><p>It isn&#39;t just ChainRules, it is everyone. Internally ChainRules tries to be consistent. Help with that is always welcomed.</p></div></div><div class="admonition terminology"><div class="admonition-title">Primal</div><div class="admonition-text"><p>Often we will talk about something as <em>primal</em>. That means it is related to the original problem, not its derivative. For example in <code>y = foo(x)</code>, <code>foo</code> is the <em>primal</em> function, and computing <code>foo(x)</code> is doing the <em>primal</em> computation. <code>y</code> is the <em>primal</em> return, and <code>x</code> is a <em>primal</em> argument. <code>typeof(y)</code> and <code>typeof(x)</code> are both <em>primal</em> types.</p></div></div><h2><a class="nav-anchor" id="frule-and-rrule-1" href="#frule-and-rrule-1"><code>frule</code> and <code>rrule</code></a></h2><div class="admonition terminology"><div class="admonition-title">`frule` and `rrule`</div><div class="admonition-text"><p><code>frule</code> and <code>rrule</code> are ChainRules specific terms. Their exact functioning is fairly ChainRules specific, though other tools have similar functions. The core notion is sometimes called <em>custom AD primitives</em>, <em>custom adjoints</em>, <em>custom gradients</em>, <em>custom sensitivities</em>.</p></div></div><p>The rules are encoded as <code>frule</code>s and <code>rrule</code>s, for use in forward-mode and reverse-mode differentiation respectively.</p><p>The <code>rrule</code> for some function <code>foo</code>, which takes the positional arguments <code>args</code> and keyword arguments <code>kwargs</code>, is written:</p><pre><code class="language-julia">function rrule(::typeof(foo), args...; kwargs...)
    ...
    return y, pullback
end</code></pre><p>where <code>y</code> (the primal result) must be equal to <code>foo(args...; kwargs...)</code>. <code>pullback</code> is a function to propagate the derivative information backwards at that point. That pullback function is used like: <code>∂self, ∂args... = pullback(Δy)</code></p><p>Almost always the <em>pullback</em> will be declared locally within the <code>rrule</code>, and will be a <em>closure</em> over some of the other arguments, and potentially over the primal result too.</p><p>The <code>frule</code> is written:</p><pre><code class="language-julia">function frule((Δself, Δargs...), ::typeof(foo), args...; kwargs...)
    ...
    return y, ∂Y
end</code></pre><p>where again <code>y = foo(args; kwargs...)</code>, and <code>∂Y</code> is the result of propagating the derivative information forwards at that point. This propagation is call the pushforward. Often we will think of the <code>frule</code> as having the primal computation <code>y = foo(args...; kwargs...)</code>, and the pushforward <code>∂Y = pushforward(Δself, Δargs...)</code>, even though they are not present in seperate forms in the code.</p><div class="admonition note"><div class="admonition-title">Why `rrule` returns a pullback but `frule` doesn&#39;t return a pushforward</div><div class="admonition-text"><p>While <code>rrule</code> takes only the arguments to the original function (the primal arguments) and returns a function (the pullback) that operates with the derivative information, the <code>frule</code> does it all at once. This is because the <code>frule</code> fuses the primal computation and the pushforward. This is an optimization that allows <code>frule</code>s to contain single large operations that perform both the primal computation and the pushforward at the same time (for example solving an ODE).</p></div></div><p>This operation is only possible in forward mode (where <code>frule</code> is used) because the derivative information needed by the pushforward available with the <code>frule</code> is invoked – it is about the primal function&#39;s inputs.     In contrast, in reverse mode the derivative information needed by the pullback is about the primal function&#39;s output.     Thus the reverse mode returns the pullback function which the caller (usually an AD system) keeps hold of until derivative information about the output is available.</p><h2><a class="nav-anchor" id="The-propagators:-pushforward-and-pullback-1" href="#The-propagators:-pushforward-and-pullback-1">The propagators: pushforward and pullback</a></h2><div class="admonition terminology"><div class="admonition-title">pushforward and pullback</div><div class="admonition-text"><p><em>Pushforward</em> and <em>pullback</em> are fancy words that the autodiff community recently adopted from Differential Geometry. The are broadly in agreement with the use of <a href="https://en.wikipedia.org/wiki/Pullback_(differential_geometry)">pullback</a> and <a href="https://en.wikipedia.org/wiki/Pushforward_(differential)">pushforward</a> in differential geometry. But any geometer will tell you these are the super-boring flat cases. Some will also frown at you. They are also sometimes described in terms of the jacobian: The <em>pushforward</em> is <em>jacobian vector product</em> (<code>jvp</code>), and <em>pullback</em> is <em>jacobian transpose vector product</em> (<code>j&#39;vp</code>). Other terms that may be used include for <em>pullback</em> the <strong>backpropagator</strong>, and by analogy for <em>pushforward</em> the <strong>forwardpropagator</strong>, thus these are the <em>propagators</em>. These are also good names because effectively they propagate wiggles and wobbles through them, via the chain rule. (the term <strong>backpropagator</strong> may originate with <a href="http://www-bcl.cs.may.ie/~barak/papers/toplas-reverse.pdf">&quot;Lambda The Ultimate Backpropagator&quot;</a> by Pearlmutter and Siskind, 2008)</p></div></div><h3><a class="nav-anchor" id="Core-Idea-1" href="#Core-Idea-1">Core Idea</a></h3><h4><a class="nav-anchor" id="Less-formally-1" href="#Less-formally-1">Less formally</a></h4><ul><li>The <strong>pushforward</strong> takes a wiggle in the <em>input space</em>, and tells what wobble you would create in the output space, by passing it through the function.</li><li>The <strong>pullback</strong> takes wobbliness information with respect to the function&#39;s output, and tells the equivalent wobbliness with respect to the functions input.</li></ul><h4><a class="nav-anchor" id="More-formally-1" href="#More-formally-1">More formally</a></h4><p>The <strong>pushforward</strong> of <span>$f$</span> takes the <em>sensitivity</em> of the input of <span>$f$</span> to a quantity, and gives the <em>sensitivity</em> of the output of <span>$f$</span> to that quantity The <strong>pullback</strong> of <span>$f$</span> takes the <em>sensitivity</em> of a quantity to the output of <span>$f$</span>, and gives the <em>sensitivity</em> of that quantity to the input of <span>$f$</span>.</p><h3><a class="nav-anchor" id="Math-1" href="#Math-1">Math</a></h3><p>This is all a bit simplified by talking in 1D.</p><h4><a class="nav-anchor" id="Lighter-Math-1" href="#Lighter-Math-1">Lighter Math</a></h4><p>For a chain of expressions:</p><pre><code class="language-none">a = f(x)
b = g(a)
c = h(b)</code></pre><p>The pullback of <code>g</code>, which incorporates the knowledge of <code>∂b/∂a</code>, applies the chain rule to go from <code>∂c/∂b</code> to <code>∂c/∂a</code>.</p><p>The pushforward of <code>g</code>,  which also incorporates the knowledge of <code>∂b/∂a</code>, applies the chain rule to go from <code>∂a/∂x</code> to <code>∂b/∂x</code>.</p><h4><a class="nav-anchor" id="Geometric-interpretation-of-reverse-and-forwards-mode-AD-1" href="#Geometric-interpretation-of-reverse-and-forwards-mode-AD-1">Geometric interpretation of reverse and forwards mode AD</a></h4><p>Let us think of our types geometrically. In other words, elements of a type form a <em>manifold</em>. This document will explain this point of view in some detail.</p><h5><a class="nav-anchor" id="Some-terminology/conventions.-1" href="#Some-terminology/conventions.-1">Some terminology/conventions.</a></h5><p>Let <span>$p$</span> be an element of type M, which is defined by some assignment of numbers <span>$x_1,...,x_m$</span>, say <span>$(x_1,...,x_m) = (a_1,...,1_m)$</span> </p><p>A <em>function</em> <span>$f:M \to K$</span> on <span>$M$</span> is (for simplicity) a polynomial <span>$K[x_1, ... x_m]$</span></p><p>The tangent space <span>$T_pM$</span> of <span>$T$</span> at point <span>$p$</span> is the <span>$K$</span>-vector space spanned by derivations <span>$d/dx$</span>.  The tangent space acts linearly on the space of functions. They act as usual on functions. Our starting point is  that we know how to write down <span>$d/dx(f) = df/dx$</span>.</p><p>The collection of tangent spaces <span>${T_pM}$</span> for <span>$p\in M$</span> is called the <em>tangent bundle</em> of <span>$M$</span>.</p><p>Let <span>$df$</span> denote the first order information of <span>$f$</span> at each point. This is called the differential of <span>$f$</span>.  If the derivatives of <span>$f$</span> and <span>$g$</span> agree at <span>$p$</span>, we say that <span>$df$</span> and <span>$dg$</span> represent the same cotangent at <span>$p$</span>. The covectors <span>$dx_1, ..., dx_m$</span> form the basis of the cotangent space <span>$T^*_pM$</span> at <span>$p$</span>. Notice that this vector space is  dual to <span>$T_p$</span></p><p>The collection of cotangent spaces <span>${T^*_pM}$</span> for <span>$p\in M$</span> is called the <em>cotangent bundle</em> of <span>$M$</span>.</p><h5><a class="nav-anchor" id="Push-forwards-and-pullbacks-1" href="#Push-forwards-and-pullbacks-1">Push-forwards and pullbacks</a></h5><p>Let <span>$N$</span> be another type, defined by numbers <span>$y_1,...,y_n$</span>, and let <span>$g:M \to N$</span> be a <em>map</em>, that is,  an <span>$n$</span>-dimensional vector <span>$(g_1, ..., g_m)$</span> of functions on <span>$M$</span>.</p><p>We define the <em>push-forward</em> <span>$g_*:TM \to TN$</span> between tangent bundles by <span>$g_*(X)(h) = X(g\circ h)$</span> for any tangent vector <span>$X$</span> and function <span>$f$</span>. We have <span>$g_*(d/dx_i)(y_j) = dg_j/dx_i$</span>, so the push-forward corresponds to the Jacobian, given a chosen basis.</p><p>Similarly, the pullback of the differential <span>$df$</span> is defined by <span>$g^*(df) = d(f\circ g)$</span>. So for a coordinate differential <span>$dy_j$</span>, we have <span>$g^*(dy_j) = d(g_j)$</span>. Notice that this is a covector, and we could have defined the pullback by its action on vectors by <span>$g^*(dh)(X) = g_*(X)(dh) = X(g\circ h)$</span> for any function <span>$f$</span> on <span>$N$</span> and <span>$X\in TM$</span>. In particular,  <span>$g^*(dy_j)(d/dx_i) = d(g_j)/dx_i$</span>. If you work out the action in a basis of the cotangent space, you see that it acts by the adjoint of the Jacobian.</p><p>Notice that the pullback of a differential and the pushforward of a vector have a very different meaning, and this should be reflected on how they are used in code.</p><p>The information contained in the push-forward map is exactly <em>what does my function do to tangent vectors</em>. Pullbacks, acting on differentials of functions, act by taking the total derivative of a function. This works in a coordinate invariant way, and works without the notion of a metric. <em>Gradients</em> recall are vectors, yet they should contain the same information of the differential <span>$df$</span>. Assuming we use the standard euclidean metric, we can identify <span>$df$</span> and <span>$\nabla f$</span> as vectors. But pulling back gradients still should not be a thing.</p><p>If the goal is to evaluate the gradient of a function <span>$f=g\circ h:M \to N \to K$</span>, where <span>$g$</span> is a map and <span>$h$</span> is a function, we have two obvious options: First, we may push-forward a basis of <span>$M$</span> to <span>$TK$</span> which we identify with K itself.  This results in <span>$m$</span> scalars, representing components of the gradient. Step-by-step in coordinates:</p><ol><li>Compute the push-forward of the basis of <span>$T_pM$</span>, i.e. just the columns of the Jacobian <span>$dg_i/dx_j$</span>.</li><li>Compute the push-forward of the function <span>$h$</span> (consider it as a map, K is also a manifold!) to get <span>$h_*(g_*T_pM) = \sum_j dh/dy_i (dg_i/dx_j)$</span></li></ol><p>Second, we pull back the differential <span>$dh$</span>: </p><ol><li>compute <span>$dh = dh/dy_1,...,dh/dy_n$</span> in coordinates.</li><li>pull back by (in coordinates) multiplying with the adjoint of the Jacobian, resulting in <span>$g_*(dh) = \sum_i(dg_i/dx_j)(dh/dy_i)$</span>.</li></ol><h3><a class="nav-anchor" id="The-anatomy-of-pullback-and-pushforward-1" href="#The-anatomy-of-pullback-and-pushforward-1">The anatomy of pullback and pushforward</a></h3><p>For our function <code>foo(args...; kwargs...) = y</code>:</p><pre><code class="language-julia">function pullback(Δy)
    ...
    return ∂self, ∂args...
end</code></pre><p>The input to the pullback is often called the <em>seed</em>. If the function is <code>y = f(x)</code> often the pullback will be written <code>s̄elf, x̄ = pullback(ȳ)</code>.</p><div class="admonition note"><div class="admonition-title">Note</div><div class="admonition-text"><p>The pullback returns one <code>∂arg</code> per <code>arg</code> to the original function, plus one <code>∂self</code> for the fields of the function itself (explained below).</p></div></div><div class="admonition terminology"><div class="admonition-title">perturbation, seed, sensitivity</div><div class="admonition-text"><p>Sometimes <em>perturbation</em>, <em>seed</em>, and even <em>sensitivity</em> will be used interchangeably. They are not generally synonymous, and ChainRules shouldn&#39;t mix them up. One must be careful when reading literature. At the end of the day, they are all <em>wiggles</em> or <em>wobbles</em>.</p></div></div><p>The pushforward is a part of the <code>frule</code> function. Considered alone it would look like:</p><pre><code class="language-julia">function pushforward(Δself, Δargs...)
    ...
    return ∂y
end</code></pre><p>But because it is fused into frule we see it as part of:</p><pre><code class="language-julia">function frule((Δself, Δargs...), ::typeof(foo), args...; kwargs...)
    ...
    return y, ∂y
end</code></pre><p>The input to the pushforward is often called the <em>perturbation</em>. If the function is <code>y = f(x)</code> often the pushforward will be written <code>ẏ = last(frule((ṡelf, ẋ), f, x))</code>. <code>ẏ</code> is commonly used to represent the perturbation for <code>y</code>.</p><div class="admonition note"><div class="admonition-title">Note</div><div class="admonition-text"><p>In the <code>frule</code>/pushforward, there is one <code>Δarg</code> per <code>arg</code> to the original function. The <code>Δargs</code> are similar in type/structure to the corresponding inputs <code>args</code> (<code>Δself</code> is explained below). The <code>∂y</code> are similar in type/structure to the original function&#39;s output <code>Y</code>. In particular if that function returned a tuple then <code>∂y</code> will be a tuple of the same size.</p></div></div><h3><a class="nav-anchor" id="Self-derivative-Δself,-self,-self,-ṡelf-etc.-1" href="#Self-derivative-Δself,-self,-self,-ṡelf-etc.-1">Self derivative <code>Δself</code>, <code>∂self</code>, <code>s̄elf</code>, <code>ṡelf</code> etc.</a></h3><div class="admonition terminology"><div class="admonition-title">Δself, ∂self, s̄elf, ṡelf</div><div class="admonition-text"><p>It is the derivatives with respect to the internal fields of the function. To the best of our knowledge there is no standard terminology for this. Other good names might be <code>Δinternal</code>/<code>∂internal</code>.</p></div></div><p>From the mathematical perspective, one may have been wondering what all this <code>Δself</code>, <code>∂self</code> is. Given that a function with two inputs, say <code>f(a, b)</code>, only has two partial derivatives: <span>$\dfrac{∂f}{∂a}$</span>, <span>$\dfrac{∂f}{∂b}$</span>. Why then does a <code>pushforward</code> take in this extra <code>Δself</code>, and why does a <code>pullback</code> return this extra <code>∂self</code>?</p><p>The reason is that in Julia the function <code>f</code> may itself have internal fields. For example a closure has the fields it closes over; a callable object (i.e. a functor) like a <code>Flux.Dense</code> has the fields of that object.</p><p><strong>Thus every function is treated as having the extra implicit argument <code>self</code>, which captures those fields.</strong> So every <code>pushforward</code> takes in an extra argument, which is ignored unless the original function has fields. It is common to write <code>function foo_pushforward(_, Δargs...)</code> in the case when <code>foo</code> does not have fields. Similarly every <code>pullback</code> returns an extra <code>∂self</code>, which for things without fields is the constant <code>NO_FIELDS</code>, indicating there are no fields within the function itself.</p><h3><a class="nav-anchor" id="Pushforward-/-Pullback-summary-1" href="#Pushforward-/-Pullback-summary-1">Pushforward / Pullback summary</a></h3><ul><li><p><strong>Pullback</strong></p><ul><li>returned by <code>rrule</code></li><li>takes output space wobbles, gives input space wiggles</li><li>1 argument per original function return</li><li>1 return per original function argument + 1 for the function itself</li></ul></li><li><p><strong>Pushforward:</strong></p><ul><li>part of <code>frule</code></li><li>takes input space wiggles, gives output space wobbles</li><li>1 argument per original function argument + 1 for the function itself</li><li>1 return per original function return</li></ul></li></ul><h3><a class="nav-anchor" id="Pullback/Pushforward-and-Directional-Derivative/Gradient-1" href="#Pullback/Pushforward-and-Directional-Derivative/Gradient-1">Pullback/Pushforward and Directional Derivative/Gradient</a></h3><p>The most trivial use of the <code>pushforward</code> from within <code>frule</code> is to calculate the <a href="https://en.wikipedia.org/wiki/Directional_derivative">directional derivative</a>:</p><p>If we would like to know the directional derivative of <code>f</code> for an input change of <code>(1.5, 0.4, -1)</code></p><pre><code class="language-julia">direction = (1.5, 0.4, -1) # (ȧ, ḃ, ċ)
y, ẏ = frule((Zero(), direction...), f, a, b, c)</code></pre><p>On the basis directions one gets the partial derivatives of <code>y</code>:</p><pre><code class="language-julia">y, ∂y_∂a = frule((Zero(), 1, 0, 0), f, a, b, c)
y, ∂y_∂b = frule((Zero(), 0, 1, 0), f, a, b, c)
y, ∂y_∂c = frule((Zero(), 0, 0, 1), f, a, b, c)</code></pre><p>Similarly, the most trivial use of <code>rrule</code> and returned <code>pullback</code> is to calculate the <a href="https://en.wikipedia.org/wiki/Gradient">gradient</a>:</p><pre><code class="language-julia">y, f_pullback = rrule(f, a, b, c)
∇f = f_pullback(1)  # for appropriate `1`-like seed.
s̄elf, ā, b̄, c̄ = ∇f</code></pre><p>Then we have that <code>∇f</code> is the <em>gradient</em> of <code>f</code> at <code>(a, b, c)</code>. And we thus have the partial derivatives <span>$\overline{\mathrm{self}}, = \dfrac{∂f}{∂\mathrm{self}}$</span>, <span>$\overline{a} = \dfrac{∂f}{∂a}$</span>, <span>$\overline{b} = \dfrac{∂f}{∂b}$</span>, <span>$\overline{c} = \dfrac{∂f}{∂c}$</span>, including the and the self-partial derivative, <span>$\overline{\mathrm{self}}$</span>.</p><h2><a class="nav-anchor" id="Differentials-1" href="#Differentials-1">Differentials</a></h2><p>The values that come back from pullbacks or pushforwards are not always the same type as the input/outputs of the primal function. They are differentials, which correspond roughly to something able to represent the difference between two values of the primal types. A differential might be such a regular type, like a <code>Number</code>, or a <code>Matrix</code>, matching to the original type; or it might be one of the <a href="api.html#ChainRulesCore.AbstractDifferential"><code>AbstractDifferential</code></a> subtypes.</p><p>Differentials support a number of operations. Most importantly: <code>+</code> and <code>*</code>, which let them act as mathematical objects.</p><p>The most important <code>AbstractDifferential</code>s when getting started are the ones about avoiding work:</p><ul><li><a href="api.html#ChainRulesCore.Thunk"><code>Thunk</code></a>: this is a deferred computation. A thunk is a <a href="https://en.wikipedia.org/wiki/Thunk">word for a zero argument closure</a>. A computation wrapped in a <code>@thunk</code> doesn&#39;t get evaluated until <a href="api.html#ChainRulesCore.unthunk-Tuple{Any}"><code>unthunk</code></a> is called on the thunk. <code>unthunk</code> is a no-op on non-thunked inputs.</li><li><a href="api.html#ChainRulesCore.One"><code>One</code></a>, <a href="api.html#ChainRulesCore.Zero"><code>Zero</code></a>: There are special representations of <code>1</code> and <code>0</code>. They do great things around avoiding expanding <code>Thunks</code> in multiplication and (for <code>Zero</code>) addition.</li></ul><h3><a class="nav-anchor" id="Other-AbstractDifferentials:-1" href="#Other-AbstractDifferentials:-1">Other <code>AbstractDifferential</code>s:</a></h3><ul><li><a href="api.html#ChainRulesCore.Composite"><code>Composite{P}</code></a>: this is the differential for tuples and  structs. Use it like a <code>Tuple</code> or <code>NamedTuple</code>. The type parameter <code>P</code> is for the primal type.</li><li><a href="api.html#ChainRulesCore.DoesNotExist"><code>DoesNotExist</code></a>: Zero-like, represents that the operation on this input is not differentiable. Its primal type is normally <code>Integer</code> or <code>Bool</code>.</li><li><a href="api.html#ChainRulesCore.InplaceableThunk"><code>InplaceableThunk</code></a>: it is like a <code>Thunk</code> but it can do in-place <code>add!</code>.</li></ul><hr/><h2><a class="nav-anchor" id="Example-of-using-ChainRules-directly.-1" href="#Example-of-using-ChainRules-directly.-1">Example of using ChainRules directly.</a></h2><p>While ChainRules is largely intended as a backend for autodiff systems, it can be used directly. In fact, this can be very useful if you can constrain the code you need to differentiate to only use things that have rules defined for. This was once how all neural network code worked.</p><p>Using ChainRules directly also helps get a feel for it.</p><pre><code class="language-julia">using ChainRulesCore

function foo(x)
    a = sin(x)
    b = 0.2 + a
    c = asin(b)
    return c
end

# Define rules (alternatively get them for free via `using ChainRules`)
@scalar_rule(sin(x), cos(x))
@scalar_rule(+(x, y), (One(), One()))
@scalar_rule(asin(x), inv(sqrt(1 - x^2)))</code></pre><pre><code class="language-julia">#### Find dfoo/dx via rrules
#### First the forward pass, accumulating rules
x = 3;
a, a_pullback = rrule(sin, x);
b, b_pullback = rrule(+, 0.2, a);
c, c_pullback = rrule(asin, b)

#### Then the backward pass calculating gradients
c̄ = 1;  # ∂c/∂c
_, b̄ = c_pullback(unthunk(c̄));     # ∂c/∂b
_, _, ā = b_pullback(unthunk(b̄));  # ∂c/∂a
_, x̄ = a_pullback(unthunk(ā));     # ∂c/∂x = ∂f/∂x
unthunk(x̄)
# output
-1.0531613736418153</code></pre><pre><code class="language-julia">#### Find dfoo/dx via frules
x = 3;
ẋ = 1;  # ∂x/∂x
nofields = Zero();  # ∂self/∂self

a, ȧ = frule((nofields, ẋ), sin, x); # ∂a/∂x
b, ḃ = frule((nofields, Zero(), unthunk(ȧ)), +, 0.2, a); # ∂b/∂x = ∂b/∂a⋅∂a/∂x

c, ċ = frule((nofields, unthunk(ḃ)), asin, b); # ∂c/∂x = ∂c/∂b⋅∂b/∂x = ∂f/∂x
unthunk(ċ)
# output
-1.0531613736418153</code></pre><pre><code class="language-julia">#### Find dfoo/dx via FiniteDifferences.jl
using FiniteDifferences
central_fdm(5, 1)(foo, x)
# output
-1.0531613736418257

#### Find dfoo/dx via ForwardDiff.jl
using ForwardDiff
ForwardDiff.derivative(foo, x)
# output
-1.0531613736418153

#### Find dfoo/dx via Zygote.jl
using Zygote
Zygote.gradient(foo, x)
# output
(-1.0531613736418153,)</code></pre><footer><hr/><a class="next" href="FAQ.html"><span class="direction">Next</span><span class="title">FAQ</span></a></footer></article></body></html>
