var documenterSearchIndex = {"docs":
[{"location":"api.html#API-Documentation-1","page":"API","title":"API Documentation","text":"","category":"section"},{"location":"api.html#","page":"API","title":"API","text":"Modules = [ChainRulesCore]","category":"page"},{"location":"api.html#ChainRulesCore.NO_FIELDS","page":"API","title":"ChainRulesCore.NO_FIELDS","text":"NO_FIELDS\n\nConstant for the reverse-mode derivative with respect to a structure that has no fields. The most notable use for this is for the reverse-mode derivative with respect to the function itself, when that function is not a closure.\n\n\n\n\n\n","category":"constant"},{"location":"api.html#ChainRulesCore.AbstractZero","page":"API","title":"ChainRulesCore.AbstractZero","text":"AbstractZero <: AbstractDifferential\n\nThis is zero-like differential types. If a AD system encounter a propagator taking as input only subtypes of AbstractZero then it can stop performing any AD operations, as all propagator are linear functions, and thus the final result will be zero.\n\nAll AbstractZero subtypes are singleton types. There are two of them Zero() and DoesNotExist().\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore.Composite","page":"API","title":"ChainRulesCore.Composite","text":"Composite{P, T} <: AbstractDifferential\n\nThis type represents the differential for a struct/NamedTuple, or Tuple. P is the the corresponding primal type that this is a differential for.\n\nComposite{P} should have fields (technically properties), that match to a subset of the fields of the primal type; and each should be a differential type matching to the primal type of that field. Fields of the P that are not present in the Composite are treated as Zero.\n\nT is an implementation detail representing the backing data structure. For Tuple it will be a Tuple, and for everything else it will be a NamedTuple. It should not be passed in by user.\n\nFor Composites of Tuples, iterate and getindex are overloaded to behave similarly to for a tuple. For Composites of structs, getproperty is overloaded to allow for accessing values via comp.fieldname. Any fields not explictly present in the Composite are treated as being set to Zero(). To make a Composite have all the fields of the primal the canonicalize function is provided.\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore.DoesNotExist","page":"API","title":"ChainRulesCore.DoesNotExist","text":"DoesNotExist() <: AbstractZero\n\nThis differential indicates that the derivative does not exist. It is the differential for a Primal type that is not differentiable. Such an Integer, or Boolean (when not being used as a represention of a value that normally would be a floating point.) The only valid way to pertube such a values is to not change it at all. As such, DoesNotExist is functionally identical to Zero(), but provides additional semantic information.\n\nIf you are adding this differential to a primal then something is wrong. A optimization package making use of this might like to check for such a case.\n\n!!! note:     This does not indicate that the derivative it is not implemented,     but rather that mathematically it is not defined.\n\nThis mostly shows up as the deriviative with respect to dimension, index, or size arguments.\n\n    function rrule(fill, x, len::Int)\n        y = fill(x, len)\n        fill_pullback(ȳ) = (NO_FIELDS, @thunk(sum(Ȳ)), DoesNotExist())\n        return y, fill_pullback\n    end\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore.InplaceableThunk","page":"API","title":"ChainRulesCore.InplaceableThunk","text":"InplaceableThunk(val::Thunk, add!::Function)\n\nA wrapper for a Thunk, that allows it to define an inplace add! function.\n\nadd! should be defined such that: ithunk.add!(Δ) = Δ .+= ithunk.val but it should do this more efficently than simply doing this directly. (Otherwise one can just use a normal Thunk).\n\nMost operations on an InplaceableThunk treat it just like a normal Thunk; and destroy its inplacability.\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore.One","page":"API","title":"ChainRulesCore.One","text":" One()\n\nThe Differential which is the multiplicative identity. Basically, this represents 1.\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore.Thunk","page":"API","title":"ChainRulesCore.Thunk","text":"Thunk(()->v)\n\nA thunk is a deferred computation. It wraps a zero argument closure that when invoked returns a differential. @thunk(v) is a macro that expands into Thunk(()->v).\n\nCalling a thunk, calls the wrapped closure. externing thunks applies recursively, it also externs the differial that the closure returns. If you do not want that, then simply call the thunk\n\njulia> t = @thunk(@thunk(3))\nThunk(var\"##7#9\"())\n\njulia> extern(t)\n3\n\njulia> t()\nThunk(var\"##8#10\"())\n\njulia> t()()\n3\n\nWhen to @thunk?\n\nWhen writing rrules (and to a lesser exent frules), it is important to @thunk appropriately. Propagation rules that return multiple derivatives may not have all deriviatives used.  By @thunking the work required for each derivative, they then compute only what is needed.\n\nHow do thunks prevent work?\n\nIf we have res = pullback(...) = @thunk(f(x)), @thunk(g(x)) then if we did dx + res[1] then only f(x) would be evaluated, not g(x). Also if we did Zero() * res[1] then the result would be Zero() and f(x) would not be evaluated.\n\nSo why not thunk everything?\n\n@thunk creates a closure over the expression, which (effectively) creates a struct with a field for each variable used in the expression, and call overloaded.\n\nDo not use @thunk if this would be equal or more work than actually evaluating the expression itself.\n\nFor more details see the manual section on using thunks effectively\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore.Zero","page":"API","title":"ChainRulesCore.Zero","text":"Zero() <: AbstractZero\n\nThe additive identity for differentials. This is basically the same as 0. A derivative of Zero(). does not propagate through the primal function.\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore.canonicalize-Union{Tuple{Composite{P,#s14} where #s14<:(NamedTuple{L,T} where T<:Tuple)}, Tuple{L}, Tuple{P}} where L where P","page":"API","title":"ChainRulesCore.canonicalize","text":"canonicalize(comp::Composite{P}) -> Composite{P}\n\nReturn the canonical Composite for the primal type P. The property names of the returned Composite match the field names of the primal, and all fields of P not present in the input comp are explictly set to Zero().\n\n\n\n\n\n","category":"method"},{"location":"api.html#ChainRulesCore.extern-Tuple{Any}","page":"API","title":"ChainRulesCore.extern","text":"extern(x)\n\nMakes a best effort attempt to convert a differential into a primal value. This is not always a well-defined operation. For two reasons:\n\nIt may not be possible to determine the primal type for a given differential.\n\nFor example, Zero is a valid differential for any primal.\n\nThe primal type might not be a vector space, thus might not be a valid differential type.\n\nFor example, if the primal type is DateTime, it's not a valid differential type as two  DateTime can not be added (fun fact: Milisecond is a differential for DateTime).\n\nWhere it is defined the operation of extern for a primal type P should be extern(x) = zero(P) + x.\n\nBecause of its limitations, extern should only really be used for testing. It can be useful, if you know what you are getting out, as it recursively removes thunks, and otherwise makes outputs more consistent with finite differencing. The more useful action in general is to call +, or in the case of thunks: unthunk.\n\nNote that extern may return an alias (not necessarily a copy) to data wrapped by x, such that mutating extern(x) might mutate x itself.\n\n\n\n\n\n","category":"method"},{"location":"api.html#ChainRulesCore.frule-Tuple{Any,Vararg{Any,N} where N}","page":"API","title":"ChainRulesCore.frule","text":"frule(f, x..., ṡelf, Δx...)\n\nExpressing x as the tuple (x₁, x₂, ...), Δx as the tuple (Δx₁, Δx₂, ...), and the output tuple of f(x...) as Ω, return the tuple:\n\n(Ω, (Ω̇₁, Ω̇₂, ...))\n\nThe second return value is the propagation rule, or the pushforward. It takes in differentials corresponding to the inputs (ẋ₁, ẋ₂, ...) and ṡelf the internal values of the function (for closures).\n\nIf no method matching frule(f, x..., ṡelf, Δx...) has been defined, then return nothing.\n\nExamples:\n\nunary input, unary output scalar function:\n\njulia> dself = Zero()\nZero()\n\njulia> x = rand();\n\njulia> sinx, sin_pushforward = frule(sin, x, dself, 1)\n(0.35696518021277485, 0.9341176907197836)\n\njulia> sinx == sin(x)\ntrue\n\njulia> sin_pushforward == cos(x)\ntrue\n\nunary input, binary output scalar function:\n\njulia> x = rand();\n\njulia> sincosx, sincos_pushforward = frule(sincos, x, dself, 1);\n\njulia> sincosx == sincos(x)\ntrue\n\njulia> sincos_pushforward == (cos(x), -sin(x))\ntrue\n\nSee also: rrule, @scalar_rule\n\n\n\n\n\n","category":"method"},{"location":"api.html#ChainRulesCore.rrule-Tuple{Any,Vararg{Any,N} where N}","page":"API","title":"ChainRulesCore.rrule","text":"rrule(f, x...)\n\nExpressing x as the tuple (x₁, x₂, ...) and the output tuple of f(x...) as Ω, return the tuple:\n\n(Ω, (Ω̄₁, Ω̄₂, ...) -> (s̄elf, x̄₁, x̄₂, ...))\n\nWhere the second return value is the the propagation rule or pullback. It takes in differentials corresponding to the outputs (x̄₁, x̄₂, ...), and s̄elf, the internal values of the function itself (for closures)\n\nIf no method matching rrule(f, xs...) has been defined, then return nothing.\n\nExamples:\n\nunary input, unary output scalar function:\n\njulia> x = rand();\n\njulia> sinx, sin_pullback = rrule(sin, x);\n\njulia> sinx == sin(x)\ntrue\n\njulia> sin_pullback(1) == (NO_FIELDS, cos(x))\ntrue\n\nbinary input, unary output scalar function:\n\njulia> x, y = rand(2);\n\njulia> hypotxy, hypot_pullback = rrule(hypot, x, y);\n\njulia> hypotxy == hypot(x, y)\ntrue\n\njulia> hypot_pullback(1) == (NO_FIELDS, (x / hypot(x, y)), (y / hypot(x, y)))\ntrue\n\nSee also: frule, @scalar_rule\n\n\n\n\n\n","category":"method"},{"location":"api.html#ChainRulesCore.unthunk-Tuple{Any}","page":"API","title":"ChainRulesCore.unthunk","text":"unthunk(x)\n\nOn AbstractThunks this removes 1 layer of thunking. On any other type, it is the identity operation.\n\nIn contrast to extern this is nonrecursive.\n\n\n\n\n\n","category":"method"},{"location":"api.html#ChainRulesCore.@scalar_rule-Tuple{Any,Any,Vararg{Any,N} where N}","page":"API","title":"ChainRulesCore.@scalar_rule","text":"@scalar_rule(f(x₁, x₂, ...),\n             @setup(statement₁, statement₂, ...),\n             (∂f₁_∂x₁, ∂f₁_∂x₂, ...),\n             (∂f₂_∂x₁, ∂f₂_∂x₂, ...),\n             ...)\n\nA convenience macro that generates simple scalar forward or reverse rules using the provided partial derivatives. Specifically, generates the corresponding methods for frule and rrule:\n\nfunction ChainRulesCore.frule(::typeof(f), x₁::Number, x₂::Number, ...)\n    Ω = f(x₁, x₂, ...)\n    $(statement₁, statement₂, ...)\n    return Ω, (_, Δx₁, Δx₂, ...) -> (\n            (∂f₁_∂x₁ * Δx₁ + ∂f₁_∂x₂ * Δx₂ + ...),\n            (∂f₂_∂x₁ * Δx₁ + ∂f₂_∂x₂ * Δx₂ + ...),\n            ...\n        )\nend\n\nfunction ChainRulesCore.rrule(::typeof(f), x₁::Number, x₂::Number, ...)\n    Ω = f(x₁, x₂, ...)\n    $(statement₁, statement₂, ...)\n    return Ω, (ΔΩ₁, ΔΩ₂, ...) -> (\n            NO_FIELDS,\n            ∂f₁_∂x₁ * ΔΩ₁ + ∂f₂_∂x₁ * ΔΩ₂ + ...),\n            ∂f₁_∂x₂ * ΔΩ₁ + ∂f₂_∂x₂ * ΔΩ₂ + ...),\n            ...\n        )\nend\n\nIf no type constraints in f(x₁, x₂, ...) within the call to @scalar_rule are provided, each parameter in the resulting frule/rrule definition is given a type constraint of Number. Constraints may also be explicitly be provided to override the Number constraint, e.g. f(x₁::Complex, x₂), which will constrain x₁ to Complex and x₂ to Number.\n\nAt present this does not support defining for closures/functors. Thus in reverse-mode, the first returned partial, representing the derivative with respect to the function itself, is always NO_FIELDS. And in forward-mode, the first input to the returned propagator is always ignored.\n\nThe result of f(x₁, x₂, ...) is automatically bound to Ω. This allows the primal result to be conveniently referenced (as Ω) within the derivative/setup expressions.\n\nThe @setup argument can be elided if no setup code is need. In other words:\n\n@scalar_rule(f(x₁, x₂, ...),\n             (∂f₁_∂x₁, ∂f₁_∂x₂, ...),\n             (∂f₂_∂x₁, ∂f₂_∂x₂, ...),\n             ...)\n\nis equivalent to:\n\n@scalar_rule(f(x₁, x₂, ...),\n             @setup(nothing),\n             (∂f₁_∂x₁, ∂f₁_∂x₂, ...),\n             (∂f₂_∂x₁, ∂f₂_∂x₂, ...),\n             ...)\n\nFor examples, see ChainRules' rulesets directory.\n\nSee also: frule, rrule.\n\n\n\n\n\n","category":"macro"},{"location":"api.html#ChainRulesCore.@thunk-Tuple{Any}","page":"API","title":"ChainRulesCore.@thunk","text":"@thunk expr\n\nDefine a Thunk wrapping the expr, to lazily defer its evaluation.\n\n\n\n\n\n","category":"macro"},{"location":"api.html#ChainRulesCore.AbstractDifferential","page":"API","title":"ChainRulesCore.AbstractDifferential","text":"The subtypes of AbstractDifferential define a custom \"algebra\" for chain rule evaluation that attempts to factor various features like complex derivative support, broadcast fusion, zero-elision, etc. into nicely separated parts.\n\nAll subtypes of AbstractDifferential implement the following operations:\n\n+(a, b): linearly combine differential a and differential b\n\n*(a, b): multiply the differential b by the scaling factor a\n\nBase.conj(x): complex conjugate of the differential x\n\nBase.zero(x) = Zero(): a zero.\n\nIn general a differential type is the type of a derivative of a value. The type of the value is for contrast called the primal type. Differential types correspond to primal types, although the relation is not one-to-one. Subtypes of  AbstractDifferential are not the only differential types. In fact for the most common primal types, such as Real or AbstractArray{Real} the the differential type is the same as the primal type.\n\nIn a circular definition: the most important property of a differential is that it should be able to be added (by defining +) to another differential of the same primal type. That allows for gradients to be accumulated.\n\nIt generally also should be able to be added to a primal to give back another primal, as this facilitates gradient descent.\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore._normalize_scalarrules_macro_input-Tuple{Any,Any,Any}","page":"API","title":"ChainRulesCore._normalize_scalarrules_macro_input","text":"_normalize_scalarrules_macro_input(call, maybe_setup, partials)\n\nreturns (in order) the correctly escaped:     - call with out any type constraints     - setup_stmts: the content of @setup or nothing if that is not provided,     -  inputs: with all args having the constraints removed from call, or         defaulting to Number     - partials: which are all Expr{:tuple,...}\n\n\n\n\n\n","category":"method"},{"location":"api.html#ChainRulesCore._zeroed_backing-Union{Tuple{Type{P}}, Tuple{P}} where P","page":"API","title":"ChainRulesCore._zeroed_backing","text":"_zeroed_backing(P)\n\nReturns a NamedTuple with same fields as P, and all values Zero().\n\n\n\n\n\n","category":"method"},{"location":"api.html#ChainRulesCore.backing-Tuple{Tuple}","page":"API","title":"ChainRulesCore.backing","text":"backing(x)\n\nAccesses the backing field of a Composite, or destructures any other composite type into a NamedTuple. Identity function on Tuple. and NamedTuples.\n\nThis is an internal function used to simplify operations between Composites and the primal types.\n\n\n\n\n\n","category":"method"},{"location":"api.html#ChainRulesCore.construct-Union{Tuple{L}, Tuple{T}, Tuple{Type{T},NamedTuple{L,T} where T<:Tuple}} where L where T","page":"API","title":"ChainRulesCore.construct","text":"construct(::Type{T}, fields::[NamedTuple|Tuple])\n\nConstructs an object of type T, with the given fields. Fields must be correct in name and type, and T must have a default constructor.\n\nThis internally is called to construct structs of the primal type T, after an operation such as the addition of a primal to a composite.\n\nIt should be overloaded, if T does not have a default constructor, or if T needs to maintain some invarients between its fields.\n\n\n\n\n\n","category":"method"},{"location":"api.html#ChainRulesCore.propagation_expr-Tuple{Any,Any}","page":"API","title":"ChainRulesCore.propagation_expr","text":"propagation_expr(Δs, ∂s)\n\nReturns the expression for the propagation of\nthe input gradient `Δs` though the partials `∂s`.\n\n\n\n\n\n","category":"method"},{"location":"api.html#ChainRulesCore.propagator_name-Tuple{Expr,Symbol}","page":"API","title":"ChainRulesCore.propagator_name","text":"propagator_name(f, propname)\n\nDetermines a reasonable name for the propagator function. The name doesn't really matter too much as it is a local function to be returned by frule or rrule, but a good name make debugging easier. f should be some form of AST representation of the actual function, propname should be either :pullback or :pushforward\n\nThis is able to deal with fairly complex expressions for f:\n\njulia> propagator_name(:bar, :pushforward)\n:bar_pushforward\n\njulia> propagator_name(esc(:(Base.Random.foo)), :pullback)\n:foo_pullback\n\n\n\n\n\n","category":"method"},{"location":"writing_good_rules.html#On-writing-good-rrule-/-frule-methods-1","page":"Writing Good Rules","title":"On writing good rrule / frule methods","text":"","category":"section"},{"location":"writing_good_rules.html#Use-Zero()-or-One()-as-return-value-1","page":"Writing Good Rules","title":"Use Zero() or One() as return value","text":"","category":"section"},{"location":"writing_good_rules.html#","page":"Writing Good Rules","title":"Writing Good Rules","text":"The Zero() and One() differential objects exist as an alternative to directly returning 0 or zeros(n), and 1 or I. They allow more optimal computation when chaining pullbacks/pushforwards, to avoid work. They should be used where possible.","category":"page"},{"location":"writing_good_rules.html#Use-Thunks-appropriately-1","page":"Writing Good Rules","title":"Use Thunks appropriately","text":"","category":"section"},{"location":"writing_good_rules.html#","page":"Writing Good Rules","title":"Writing Good Rules","text":"If work is only required for one of the returned differentials, then it should be wrapped in a @thunk (potentially using a begin-end block).","category":"page"},{"location":"writing_good_rules.html#","page":"Writing Good Rules","title":"Writing Good Rules","text":"If there are multiple return values, their computation should almost always be wrapped in a @thunk.","category":"page"},{"location":"writing_good_rules.html#","page":"Writing Good Rules","title":"Writing Good Rules","text":"Do not wrap variables in a @thunk; wrap the computations that fill those variables in @thunk:","category":"page"},{"location":"writing_good_rules.html#","page":"Writing Good Rules","title":"Writing Good Rules","text":"# good:\n∂A = @thunk(foo(x))\nreturn ∂A\n\n# bad:\n∂A = foo(x)\nreturn @thunk(∂A)","category":"page"},{"location":"writing_good_rules.html#","page":"Writing Good Rules","title":"Writing Good Rules","text":"In the bad example foo(x) gets computed eagerly, and all that the thunk is doing is wrapping the already calculated result in a function that returns it.","category":"page"},{"location":"writing_good_rules.html#","page":"Writing Good Rules","title":"Writing Good Rules","text":"Do not use @thunk if this would be equal or more work than actually evaluating the expression itself. Examples being:","category":"page"},{"location":"writing_good_rules.html#","page":"Writing Good Rules","title":"Writing Good Rules","text":"The expression being a constant\nThe expression is merely wrapping something in a struct, such as Adjoint(x) or Diagonal(x)\nThe expression being itself a thunk\nThe expression being from another rrule or frule; it would be @thunked if required by the defining rule already.\nThere is only one derivative being returned, so from the fact that the user called frule/rrule they clearly will want to use that one.","category":"page"},{"location":"writing_good_rules.html#Be-careful-with-using-adjoint-when-you-mean-transpose-1","page":"Writing Good Rules","title":"Be careful with using adjoint when you mean transpose","text":"","category":"section"},{"location":"writing_good_rules.html#","page":"Writing Good Rules","title":"Writing Good Rules","text":"Remember for complex numbers a' (i.e. adjoint(a)) takes the complex conjugate. Instead you probably want transpose(a), unless you've already restricted a to be a AbstractMatrix{<:Real}.","category":"page"},{"location":"writing_good_rules.html#Code-Style-1","page":"Writing Good Rules","title":"Code Style","text":"","category":"section"},{"location":"writing_good_rules.html#","page":"Writing Good Rules","title":"Writing Good Rules","text":"Use named local functions for the pushforward/pullback:","category":"page"},{"location":"writing_good_rules.html#","page":"Writing Good Rules","title":"Writing Good Rules","text":"# good:\nfunction frule(::typeof(foo), x)\n    Y = foo(x)\n    function foo_pushforward(_, ẋ)\n        return bar(ẋ)\n    end\n    return Y, foo_pushforward\nend\n#== output\njulia> frule(foo, 2)\n(4, var\"#foo_pushforward#11\"())\n==#\n\n# bad:\nfunction frule(::typeof(foo), x)\n    return foo(x), (_, ẋ) -> bar(ẋ)\nend\n#== output:\njulia> frule(foo, 2)\n(4, var\"##9#10\"())\n==#","category":"page"},{"location":"writing_good_rules.html#","page":"Writing Good Rules","title":"Writing Good Rules","text":"While this is more verbose, it ensures that if an error is thrown during the pullback/pushforward the gensym name of the local function will include the name you gave it. This makes it a lot simpler to debug from the stacktrace.","category":"page"},{"location":"writing_good_rules.html#Write-tests-1","page":"Writing Good Rules","title":"Write tests","text":"","category":"section"},{"location":"writing_good_rules.html#","page":"Writing Good Rules","title":"Writing Good Rules","text":"There are fairly decent tools for writing tests based on FiniteDifferences.jl. They are in tests/test_utils.jl. Take a look at existing test and you should see how to do stuff.","category":"page"},{"location":"writing_good_rules.html#","page":"Writing Good Rules","title":"Writing Good Rules","text":"warning: Warning\nUse finite differencing to test derivatives. Don't use analytical derivations for derivatives in the tests. Those are what you use to define the rules, and so can not be confidently used in the test. If you misread/misunderstood them, then your tests/implementation will have the same mistake.","category":"page"},{"location":"writing_good_rules.html#CAS-systems-are-your-friends.-1","page":"Writing Good Rules","title":"CAS systems are your friends.","text":"","category":"section"},{"location":"writing_good_rules.html#","page":"Writing Good Rules","title":"Writing Good Rules","text":"It is very easy to check gradients or derivatives with a computer algebra system (CAS) like WolframAlpha.","category":"page"},{"location":"FAQ.html#FAQ-1","page":"FAQ","title":"FAQ","text":"","category":"section"},{"location":"FAQ.html#What-is-up-with-the-different-symbols?-1","page":"FAQ","title":"What is up with the different symbols?","text":"","category":"section"},{"location":"FAQ.html#Δx,-x,-dx-1","page":"FAQ","title":"Δx, ∂x, dx","text":"","category":"section"},{"location":"FAQ.html#","page":"FAQ","title":"FAQ","text":"ChainRules uses these perhaps atypically. As a notation that is the same across propagators, regardless of direction (incontrast see ẋ and x̄ below).","category":"page"},{"location":"FAQ.html#","page":"FAQ","title":"FAQ","text":"Δx is the input to a propagator, (i.e a seed for a pullback; or a perturbation for a pushforward)\n∂x is the output of a propagator\ndx could be either input or output","category":"page"},{"location":"FAQ.html#dots-and-bars:-\\dot{y}-\\dfrac{y}{x}-\\overline{x}-1","page":"FAQ","title":"dots and bars: doty = dfracyx = overlinex","text":"","category":"section"},{"location":"FAQ.html#","page":"FAQ","title":"FAQ","text":"v̇ is a derivative of the input moving forward: v = fracvx for input x, intermediate value v.\nv̄ is a derivative of the output moving backward: v = fracyv for output y, intermediate value v.","category":"page"},{"location":"FAQ.html#others-1","page":"FAQ","title":"others","text":"","category":"section"},{"location":"FAQ.html#","page":"FAQ","title":"FAQ","text":"Ω is often used as the return value of the function. Especially, but not exclusively, for scalar functions.\nΔΩ is thus a seed for the pullback.\n∂Ω is thus the output of a pushforward.","category":"page"},{"location":"FAQ.html#Why-does-rrule-return-the-primal-function-evaluation?-1","page":"FAQ","title":"Why does rrule return the primal function evaluation?","text":"","category":"section"},{"location":"FAQ.html#","page":"FAQ","title":"FAQ","text":"You might wonder why frule(f, x) returns f(x) and the derivative of f at x, and similarly for rrule returning f(x) and the pullback for f at x. Why not just return the pushforward/pullback, and let the user call f(x) to get the answer separately?","category":"page"},{"location":"FAQ.html#","page":"FAQ","title":"FAQ","text":"There are three reasons the rules also calculate the f(x).","category":"page"},{"location":"FAQ.html#","page":"FAQ","title":"FAQ","text":"For some rules an alternative way of calculating f(x) can give the same answer while also generating intermediate values that can be used in the calculations required to propagate the derivative.\nFor many rrules the output value is used in the definition of the pullback. For example tan, sigmoid etc.\nFor some frules there exists a single, non-separable operation that will compute both derivative and primal result. For example many of the methods for differential equation sensitivity analysis.","category":"page"},{"location":"FAQ.html#Where-are-the-derivatives-for-keyword-arguments?-1","page":"FAQ","title":"Where are the derivatives for keyword arguments?","text":"","category":"section"},{"location":"FAQ.html#","page":"FAQ","title":"FAQ","text":"pullbacks do not return a sensitivity for keyword arguments; similarly pushfowards do not accept a perturbation for keyword arguments. This is because in practice functions are very rarely differentiable with respect to keyword arguments. As a rule keyword arguments tend to control side-effects, like logging verbosity, or to be functionality changing to perform a different operation, e.g. dims=3, and thus not differentiable. To the best of our knowledge no Julia AD system, with support for the definition of custom primitives, supports differentiating with respect to keyword arguments. At some point in the future ChainRules may support these. Maybe.","category":"page"},{"location":"FAQ.html#What-is-the-difference-between-Zero-and-DoesNotExist-?-1","page":"FAQ","title":"What is the difference between Zero and DoesNotExist ?","text":"","category":"section"},{"location":"FAQ.html#","page":"FAQ","title":"FAQ","text":"Zero and DoesNotExist act almost exactly the same in practice: they result in no change whenever added to anything. Odds are if you write a rule that returns the wrong one everything will just work fine. We provide both to allow for clearer writing of rules, and easier debugging.","category":"page"},{"location":"FAQ.html#","page":"FAQ","title":"FAQ","text":"Zero() represents the fact that if one perturbs (adds a small change to) the matching primal there will be no change in the behavour of the primal function. For example in fst(x,y) = x, then the derivative of fst with respect to y is Zero(). fst(10, 5) == 10 and if we add 0.1 to 5 we still get fst(10, 5.1)=10.","category":"page"},{"location":"FAQ.html#","page":"FAQ","title":"FAQ","text":"DoesNotExist() represents the fact that if one perturbs the matching primal, the primal function will now error. For example in access(xs, n) = xs[n] then the derivative of access with respect to n is DoesNotExist(). access([10, 20, 30], 2) = 20, but if we add 0.1 to 2 we get access([10, 20, 30], 2.1) which errors as indexing can't be applied at fractional indexes.","category":"page"},{"location":"FAQ.html#When-to-use-ChainRules-vs-ChainRulesCore?-1","page":"FAQ","title":"When to use ChainRules vs ChainRulesCore?","text":"","category":"section"},{"location":"FAQ.html#","page":"FAQ","title":"FAQ","text":"ChainRulesCore.jl is a light-weight dependency for defining rules for functions in your packages, without you needing to depend on ChainRules.jl itself. It has no dependencies of its own. If you only want to define rules, not use them, then you probably only want to load ChainRulesCore.jl.","category":"page"},{"location":"FAQ.html#","page":"FAQ","title":"FAQ","text":"ChainRules.jl provides the full functionality for AD systems, in particular it has all the rules for Base Julia and the standard libraries. It is thus a much heavier package to load. AD systems making use of frules and rrule should load ChainRules.jl.","category":"page"},{"location":"FAQ.html#Where-should-I-put-my-rules?-1","page":"FAQ","title":"Where should I put my rules?","text":"","category":"section"},{"location":"FAQ.html#","page":"FAQ","title":"FAQ","text":"We recommend adding custom rules to your own packages with ChainRulesCore.jl, rather than adding them to ChainRules.jl. A few packages - currently SpecialFunctions.jl and NaNMath.jl - have rules in ChainRules.jl as a short-term measure.","category":"page"},{"location":"FAQ.html#","page":"FAQ","title":"FAQ","text":"You can use ChainRulesTestUtils.jl to test your custom rules. ChainRulesTestUtils.jl has some dependencies, so it is a separate package from ChainRulesCore.jl. This means your package can depend on the light-weight ChainRulesCore.jl, and make ChainRulesTestUtils.jl a test-only dependency.","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"DocTestSetup = :(using ChainRulesCore, ChainRules)","category":"page"},{"location":"index.html#ChainRules-1","page":"Introduction","title":"ChainRules","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"ChainRules provides a variety of common utilities that can be used by downstream automatic differentiation (AD) tools to define and execute forward-, reverse-, and mixed-mode primitives.","category":"page"},{"location":"index.html#Introduction-1","page":"Introduction","title":"Introduction","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"ChainRules is all about providing a rich set of rules for differentiation. When a person learns introductory calculus, they learn that the derivative (with respect to x) of a*x is a, and the derivative of sin(x) is cos(x), etc. And they learn how to combine simple rules, via the chain rule, to differentiate complicated functions. ChainRules is a programmatic repository of that knowledge, with the generalizations to higher dimensions.","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"Autodiff (AD) tools roughly work by reducing a problem down to simple parts that they know the rules for, and then combining those rules. Knowing rules for more complicated functions speeds up the autodiff process as it doesn't have to break things down as much.","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"ChainRules is an AD-independent collection of rules to use in a differentiation system.","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"note: The whole field is a mess for terminology\nIt isn't just ChainRules, it is everyone. Internally ChainRules tries to be consistent. Help with that is always welcomed.","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"terminology: Primal\nOften we will talk about something as primal. That means it is related to the original problem, not its derivative. For example in y = foo(x), foo is the primal function, and computing foo(x) is doing the primal computation. y is the primal return, and x is a primal argument. typeof(y) and typeof(x) are both primal types.","category":"page"},{"location":"index.html#frule-and-rrule-1","page":"Introduction","title":"frule and rrule","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"terminology: `frule` and `rrule`\nfrule and rrule are ChainRules specific terms. Their exact functioning is fairly ChainRules specific, though other tools have similar functions. The core notion is sometimes called custom AD primitives, custom adjoints, customgradients, _custom sensitivities.","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"The rules are encoded as frules and rrules, for use in forward-mode and reverse-mode differentiation respectively.","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"The rrule for some function foo, which takes the positional arguments args and keyword arguments kwargs, is written:","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"function rrule(::typeof(foo), args...; kwargs...)\n    ...\n    return y, pullback\nend","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"where y (the primal result) must be equal to foo(args...; kwargs...). pullback is a function to propagate the derivative information backwards at that point. That pullback function is used like: ∂self, ∂args... = pullback(Δy)","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"Almost always the pullback will be declared locally within the rrule, and will be a closure over some of the other arguments, and potentially over the primal result too.","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"The frule is written:","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"function frule(::typeof(foo), args..., Δself, Δargs...; kwargs...)\n    ...\n    return y, ∂Y\nend","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"where again y = foo(args; kwargs...), and ∂Y is the result of propagating the derivative information forwards at that point. This propagation is call the pushforward. One could think of writing ∂Y = pushforward(Δself, Δargs), and often we will think of the frule as having the primal computation y = foo(args...; kwargs...), and the push-forward ∂Y = pushforward(Δself, Δargs...)","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"note: Why `rrule` returns a pullback but `frule` doesn't return a pushforward\nWhile rrule takes only the arguments to the original function (the primal arguments) and returns a function (the pullback) that operates with the derivative information, the frule does it all at once. This is because the frule fuses the primal computation and the pushforward. This is an optimization that allows frules to contain single large operations that perform both the primal computation and the pushforward at the same time (for example solving an ODE).","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"This operation is only possible in forward mode (where frule is used) because the derivative information needed by the pushforward available with the frule is invoked – it is about the primal function's inputs.     In contrast, in reverse mode the derivative information needed by the pullback is about the primal function's output.     Thus the reverse mode returns the pullback function which the caller (usually an AD system) keeps hold of until derivative information about the output is available.","category":"page"},{"location":"index.html#The-propagators:-pushforward-and-pullback-1","page":"Introduction","title":"The propagators: pushforward and pullback","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"terminology: pushforward and pullback\nPushforward and pullback are fancy words that the autodiff community recently adopted from Differential Geometry. The are broadly in agreement with the use of pullback and pushforward in differential geometry. But any geometer will tell you these are the super-boring flat cases. Some will also frown at you. They are also sometimes described in terms of the jacobian: The pushforward is jacobian vector product (jvp), and pullback is jacobian transpose vector product (j'vp). Other terms that may be used include for pullback the backpropagator, and by analogy for pushforward the forwardpropagator, thus these are the propagators. These are also good names because effectively they propagate wiggles and wobbles through them, via the chain rule. (the term backpropagator may originate with \"Lambda The Ultimate Backpropagator\" by Pearlmutter and Siskind, 2008)","category":"page"},{"location":"index.html#Core-Idea-1","page":"Introduction","title":"Core Idea","text":"","category":"section"},{"location":"index.html#Less-formally-1","page":"Introduction","title":"Less formally","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"The pushforward takes a wiggle in the input space, and tells what wobble you would create in the output space, by passing it through the function.\nThe pullback takes wobbliness information with respect to the function's output, and tells the equivalent wobbliness with respect to the functions input.","category":"page"},{"location":"index.html#More-formally-1","page":"Introduction","title":"More formally","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"The pushforward of f takes the sensitivity of the input of f to a quantity, and gives the sensitivity of the output of f to that quantity The pullback of f takes the sensitivity of a quantity to the output of f, and gives the sensitivity of that quantity to the input of f.","category":"page"},{"location":"index.html#Math-1","page":"Introduction","title":"Math","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"This is all a bit simplified by talking in 1D.","category":"page"},{"location":"index.html#Lighter-Math-1","page":"Introduction","title":"Lighter Math","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"For a chain of expressions:","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"a = f(x)\nb = g(a)\nc = h(b)","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"The pullback of g, which incorporates the knowledge of ∂b/∂a, applies the chain rule to go from ∂c/∂b to ∂c/∂a.","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"The pushforward of g,  which also incorporates the knowledge of ∂b/∂a, applies the chain rule to go from ∂a/∂x to ∂b/∂x.","category":"page"},{"location":"index.html#Heavier-Math-1","page":"Introduction","title":"Heavier Math","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"If I have some functions: g(a), h(b) and f(x)=g(h(x)), and I know the pullback of g, at h(x) written: mathrmpullback_g(a)a=h(x), and I know the derivative of h with respect to its input b at g(x), written: leftdfrachbright_b=g(x) Then I can use the pullback to find: dfracfx:","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"dfracfx=mathrmmathrmpullback_g(a)a=h(x)left(leftdfrachbright_b=g(x)right)","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"If I know the derivative of g with respect to its input a at x, written: leftdfracgaright_a=x, and I know the pushforward of h at g(x) written: mathrmpushforward_h(b)b=g(x). Then I can use the pushforward to find dfracfx:","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"dfracfx=mathrmpushforward_h(b)b=g(x)left(leftdfracgaright_a=xright)","category":"page"},{"location":"index.html#The-anatomy-of-pullback-and-pushforward-1","page":"Introduction","title":"The anatomy of pullback and pushforward","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"For our function foo(args...; kwargs...) = y:","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"function pullback(Δy)\n    ...\n    return ∂self, ∂args...\nend","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"The input to the pullback is often called the seed. If the function is y = f(x) often the pullback will be written s̄elf, x̄ = pullback(ȳ).","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"note: Note\nThe pullback returns one ∂arg per arg to the original function, plus one ∂self for the fields of the function itself (explained below).","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"terminology: perturbation, seed, sensitivity\nSometimes perturbation, seed, and even sensitivity will be used interchangeably. They are not generally synonymous, and ChainRules shouldn't mix them up. One must be careful when reading literature. At the end of the day, they are all wiggles or wobbles.","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"The pushforward is a part of the frule function. Considered alone it would look like:","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"function pushforward(Δself, Δargs...)\n    ...\n    return ∂y\nend","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"But because it is fused into frule we see it as part of:","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"function frule(::typeof(foo), args..., Δself, Δargs...; kwargs...)\n    ...\n    return y, ∂y\nend","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"The input to the pushforward is often called the perturbation. If the function is y = f(x) often the pushforward will be written ẏ = last(frule(f, x, ṡelf, ẋ)). ẏ is commonly used to represent the perturbation for y.","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"note: Note\nIn the frule/pushforward, there is one Δarg per arg to the original function. The Δargs are similar in type/structure to the corresponding inputs args (Δself is explained below). The ∂y are similar in type/structure to the original function's output Y. In particular if that function returned a tuple then ∂y will be a tuple of the same size.","category":"page"},{"location":"index.html#Self-derivative-Δself,-self,-self,-ṡelf-etc.-1","page":"Introduction","title":"Self derivative Δself, ∂self, s̄elf, ṡelf etc.","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"terminology: Δself, ∂self, s̄elf, ṡelf\nIt is the derivatives with respect to the internal fields of the function. To the best of our knowledge there is no standard terminology for this. Other good names might be Δinternal/∂internal.","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"From the mathematical perspective, one may have been wondering what all this Δself, ∂self is. Given that a function with two inputs, say f(a, b), only has two partial derivatives: dfracfa, dfracfb. Why then does a pushforward take in this extra Δself, and why does a pullback return this extra ∂self?","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"The reason is that in Julia the function f may itself have internal fields. For example a closure has the fields it closes over; a callable object (i.e. a functor) like a Flux.Dense has the fields of that object.","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"Thus every function is treated as having the extra implicit argument self, which captures those fields. So every pushforward takes in an extra argument, which is ignored unless the original function has fields. It is common to write function foo_pushforward(_, Δargs...) in the case when foo does not have fields. Similarly every pullback returns an extra ∂self, which for things without fields is the constant NO_FIELDS, indicating there are no fields within the function itself.","category":"page"},{"location":"index.html#Pushforward-/-Pullback-summary-1","page":"Introduction","title":"Pushforward / Pullback summary","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"Pullback\nreturned by rrule\ntakes output space wobbles, gives input space wiggles\n1 argument per original function return\n1 return per original function argument + 1 for the function itself\nPushforward:\npart of frule\ntakes input space wiggles, gives output space wobbles\n1 argument per original function argument + 1 for the function itself\n1 return per original function return","category":"page"},{"location":"index.html#Pullback/Pushforward-and-Directional-Derivative/Gradient-1","page":"Introduction","title":"Pullback/Pushforward and Directional Derivative/Gradient","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"The most trivial use of the pushforward from within frule is to calculate the directional derivative:","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"If we would like to know the the directional derivative of f for an input change of (1.5, 0.4, -1)","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"direction = (1.5, 0.4, -1) # (ȧ, ḃ, ċ)\ny, ẏ = frule(f, a, b, c, Zero(), direction)","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"On the basis directions one gets the partial derivatives of y:","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"y, ∂y_∂a = frule(f, a, b, c, Zero(), 1, 0, 0)\ny, ∂y_∂b = frule(f, a, b, c, Zero(), 0, 1, 0)\ny, ∂y_∂c = frule(f, a, b, c, Zero(), 0, 0, 1)","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"Similarly, the most trivial use of rrule and returned pullback is to calculate the Gradient:","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"y, f_pullback = rrule(f, a, b, c)\n∇f = f_pullback(1)  # for appropriate `1`-like seed.\ns̄elf, ā, b̄, c̄ = ∇f","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"Then we have that ∇f is the gradient of f at (a, b, c). And we thus have the partial derivatives overlinemathrmself = dfracfmathrmself, overlinea = dfracfa, overlineb = dfracfb, overlinec = dfracfc, including the and the self-partial derivative, overlinemathrmself.","category":"page"},{"location":"index.html#Differentials-1","page":"Introduction","title":"Differentials","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"The values that come back from pullbacks or pushforwards are not always the same type as the input/outputs of the primal function. They are differentials, which correspond roughly to something able to represent the difference between two values of the primal types. A differential might be such a regular type, like a Number, or a Matrix, matching to the original type; or it might be one of the AbstractDifferential subtypes.","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"Differentials support a number of operations. Most importantly: + and *, which let them act as mathematical objects.","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"The most important AbstractDifferentials when getting started are the ones about avoiding work:","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"Thunk: this is a deferred computation. A thunk is a word for a zero argument closure. A computation wrapped in a @thunk doesn't get evaluated until unthunk is called on the thunk. unthunk is a no-op on non-thunked inputs.\nOne, Zero: There are special representations of 1 and 0. They do great things around avoiding expanding Thunks in multiplication and (for Zero) addition.","category":"page"},{"location":"index.html#Other-AbstractDifferentials:-1","page":"Introduction","title":"Other AbstractDifferentials:","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"Composite{P}: this is the differential for tuples and  structs. Use it like a Tuple or NamedTuple. The type parameter P is for the primal type.\nDoesNotExist: Zero-like, represents that the operation on this input is not differentiable. Its primal type is normally Integer or Bool.\nInplaceableThunk: it is like a Thunk but it can do in-place add!.","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"","category":"page"},{"location":"index.html#Example-of-using-ChainRules-directly.-1","page":"Introduction","title":"Example of using ChainRules directly.","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"While ChainRules is largely intended as a backend for autodiff systems, it can be used directly. In fact, this can be very useful if you can constrain the code you need to differentiate to only use things that have rules defined for. This was once how all neural network code worked.","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"Using ChainRules directly also helps get a feel for it.","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"using ChainRules\n\nfunction foo(x)\n    a = sin(x)\n    b = 2a\n    c = asin(b)\n    return c\nend\n\n#### Find dfoo/dx via rrules\n#### First the forward pass, accumulating rules\nx = 3;\na, a_pullback = rrule(sin, x);\nb, b_pullback = rrule(*, 2, a);\nc, c_pullback = rrule(asin, b)\n\n#### Then the backward pass calculating gradients\nc̄ = 1;  # ∂c/∂c\n_, b̄ = c_pullback(extern(c̄));     # ∂c/∂b\n_, _, ā = b_pullback(extern(b̄));  # ∂c/∂a\n_, x̄ = a_pullback(extern(ā));     # ∂c/∂x = ∂f/∂x\nextern(x̄)\n# output\n-2.0638950738662625","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"#### Find dfoo/dx via frules\nx = 3;\nẋ = 1;  # ∂x/∂x\nnofields = Zero();  # ∂self/∂self\n\na, ȧ = frule(sin, x, nofields, ẋ); # ∂a/∂x\nb, ḃ = frule(*, 2, a, nofields, Zero(), unthunk(ȧ)); # ∂b/∂x = ∂b/∂a⋅∂a/∂x\n\nc, ċ = frule(asin, b, nofields, unthunk(ḃ)); # ∂c/∂x = ∂c/∂b⋅∂b/∂x = ∂f/∂x\nunthunk(ċ)\n# output\n-2.0638950738662625","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"#### Find dfoo/dx via FiniteDifferences.jl\nusing FiniteDifferences\ncentral_fdm(5, 1)(foo, x)\n# output\n-2.0638950738670734\n\n#### Find dfoo/dx via ForwardDiff.jl\nusing ForwardDiff\nForwardDiff.derivative(foo, x)\n# output\n-2.0638950738662625\n\n#### Find dfoo/dx via Zygote.jl\nusing Zygote\nZygote.gradient(foo, x)\n# output\n(-2.0638950738662625,)","category":"page"}]
}
